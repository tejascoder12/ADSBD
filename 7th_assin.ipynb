{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN95P+djgZ3HcMw0msHoVyf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejascoder12/ADSBD/blob/main/7th_assin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "thP1LJStM0G-"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "-rO20LdANIAp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "tXmnDjzPNwYn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQfhiKO_N5vY",
        "outputId": "2efef6c7-f3ce-4436-a601-4295801b41d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"Tokenization is the first step in text analytics.Tokenization replaces a sensitive data element, for example, a bank account number, with a non-sensitive substitute, known as a token. The token is a randomized data string that has no essential or exploitable value or meaning\""
      ],
      "metadata": {
        "id": "0DW4o_-dPcv4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for sentence"
      ],
      "metadata": {
        "id": "E0g--buQayje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_text= sent_tokenize(text)\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSulxEjiPtf5",
        "outputId": "53a42355-fd1c-4ee3-a01c-3fd274500e6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization is the first step in text analytics.Tokenization replaces a sensitive data element, for example, a bank account number, with a non-sensitive substitute, known as a token.', 'The token is a randomized data string that has no essential or exploitable value or meaning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "##tokanized word\n",
        "\n"
      ],
      "metadata": {
        "id": "Ruk6BCh8R7SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for word\n"
      ],
      "metadata": {
        "id": "rZfeqOPua8b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlWgDGnYQYR4",
        "outputId": "ec396d7d-28dc-4c12-f365-bac4fda5050e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics.Tokenization', 'replaces', 'a', 'sensitive', 'data', 'element', ',', 'for', 'example', ',', 'a', 'bank', 'account', 'number', ',', 'with', 'a', 'non-sensitive', 'substitute', ',', 'known', 'as', 'a', 'token', '.', 'The', 'token', 'is', 'a', 'randomized', 'data', 'string', 'that', 'has', 'no', 'essential', 'or', 'exploitable', 'value', 'or', 'meaning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for stop word \n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\\\n",
        "\n",
        " \"a,\" \"an,\" \"the,\" \"is,\" \"and,\",etc\n",
        "  "
      ],
      "metadata": {
        "id": "eB2vf44TRgLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc0j0kOrQd9o",
        "outputId": "12a4e3ad-ffd4-4549-bfd0-dfba5dde134e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'having', 'had', 'aren', 's', 't', 'she', \"mustn't\", 'all', 'itself', 'over', 'he', 'we', \"shouldn't\", 'some', \"isn't\", 'themselves', 'so', 'it', 'been', 'now', 'doesn', 'up', 'too', 'of', 'they', 'after', \"that'll\", 'this', 'before', 'how', 'couldn', 'most', 'off', 'both', 'be', 'o', 'each', 'these', 'myself', 'theirs', 'from', 'here', 'no', 'our', 'about', 'who', 'y', 'yourself', 'then', 'more', 'to', 'than', 'ours', 'ma', 'there', 'same', 'below', 'which', 'whom', 'for', 'again', 'during', \"didn't\", 'haven', 'its', 'above', 'with', 'mightn', 'at', 'while', 'yours', 'ain', 'didn', 'me', 'd', 'against', 'needn', 'do', 'isn', 'your', 'don', 're', 'mustn', 'very', 'few', 'her', 'if', 'by', \"you've\", 'but', 'under', 'where', \"shan't\", 'won', 'between', 'his', \"couldn't\", \"you'll\", \"you'd\", 've', 'can', 'hasn', 'is', 'shan', 'm', 'as', 'yourselves', \"doesn't\", 'shouldn', 'those', 'an', \"wasn't\", 'them', \"needn't\", \"you're\", 'in', 'doing', 'were', 'not', 'll', 'or', \"don't\", \"haven't\", 'when', \"hasn't\", 'him', 'himself', 'being', 'should', 'what', 'other', 'hers', \"she's\", 'why', 'was', 'does', 'and', 'into', 'their', 'are', \"weren't\", 'until', 'hadn', 'own', \"mightn't\", \"wouldn't\", 'on', 'that', 'out', \"should've\", 'my', 'weren', 'the', 'has', 'down', \"aren't\", 'did', 'ourselves', 'wasn', 'once', 'a', 'any', \"hadn't\", 'only', 'am', 'i', 'further', 'have', 'through', 'wouldn', 'such', 'you', 'just', \"won't\", 'herself', 'will', 'because', 'nor', \"it's\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re #for matching one string with other \"re\""
      ],
      "metadata": {
        "id": "xhxCBP8qQoC4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"How to remove stop words with NLTK library in Python?\"\n",
        "text= re.sub('[^a-zA-Z]', ' ',text)\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered_text=[]\n",
        "for w in tokens:\n",
        " if w not in stop_words:\n",
        "   filtered_text.append(w)\n",
        "print(\"Tokenized Sentence:\",tokens)\n",
        "print(\"Filterd Sentence:\",filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc2fhNWdQr04",
        "outputId": "e58a4680-9a6e-4476-e530-7d1a7ddebdc8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
            "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for stemming: remove prefix and suffix"
      ],
      "metadata": {
        "id": "HLQAgbEIcHO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "ps =PorterStemmer()\n",
        "for w in e_words:\n",
        " rootWord=ps.stem(w)\n",
        "print(rootWord)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZpi1NJrQ5H5",
        "outputId": "32955b8a-5537-4c78-e030-f0ebea8d0642"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"studies studying cries cry\"\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        " print(\"Lemma for {} is {}\".format(w,\n",
        "wordnet_lemmatizer.lemmatize(w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-csdTp2mRDOo",
        "outputId": "35fcc46e-355e-471a-8c7a-808dd82325d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "data=\"The pink sweater fit her perfectly\"\n",
        "words=word_tokenize(data)\n",
        "for word in words:\n",
        " print(nltk.pos_tag([word]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo4cqmADRGs5",
        "outputId": "163b89e9-78bc-4da9-ad67-9da3c38c6504"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT')]\n",
            "[('pink', 'NN')]\n",
            "[('sweater', 'NN')]\n",
            "[('fit', 'NN')]\n",
            "[('her', 'PRP$')]\n",
            "[('perfectly', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization start"
      ],
      "metadata": {
        "id": "6eSKPl0XSbqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, peopl\n",
        " the world have come and invaded us, captured our lands, conquered ou\n",
        " Yet we have not done this to any other nation. We have not conquered\n",
        " We have not grabbed their land, their culture,\n",
        " their history and tried to enforce our way of life on them.\n",
        " Why? \"\"\"\n"
      ],
      "metadata": {
        "id": "rx7Nl59RRRwo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "mJU7Sd8dSWai"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn=WordNetLemmatizer()\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "3St3LWRbSh75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104b9982-ccfc-4b29-f3bf-cd04dbeb53f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I have three visions for India.', 'In 3000 years of our history, peopl\\n the world have come and invaded us, captured our lands, conquered ou\\n Yet we have not done this to any other nation.', 'We have not conquered\\n We have not grabbed their land, their culture,\\n their history and tried to enforce our way of life on them.', 'Why?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "corpus = []\n",
        "wn = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for sentence in sentences:\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentence) \n",
        "    review = review.lower()  \n",
        "    review = review.split() \n",
        "    review = [wn.lemmatize(word) for word in review if word not in stop_words] \n",
        "    review = ' '.join(review) \n",
        "    corpus.append(review)  \n"
      ],
      "metadata": {
        "id": "jpFjQgCrSkmp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91yoMHzSSteZ",
        "outputId": "b9c4f797-2955-4727-d103-6b9493bcee5e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three vision india',\n",
              " 'year history peopl world come invaded u captured land conquered ou yet done nation',\n",
              " 'conquered grabbed land culture history tried enforce way life',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "# Creating the Term Freq -Invers Dco Freq model\n",
        "\n"
      ],
      "metadata": {
        "id": "ossIPQEjYm9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "X=tf.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaRi6SC2T83J",
        "outputId": "f5a0a5ed-4315-40e0-aedc-c20e88da7c49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.57735027 0.         0.         0.\n",
            "  0.         0.         0.         0.57735027 0.         0.57735027\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.29031548 0.29031548 0.22888806 0.         0.29031548 0.\n",
            "  0.         0.22888806 0.         0.29031548 0.22888806 0.\n",
            "  0.29031548 0.29031548 0.29031548 0.         0.         0.\n",
            "  0.         0.29031548 0.29031548 0.29031548]\n",
            " [0.         0.         0.28113163 0.35657982 0.         0.35657982\n",
            "  0.35657982 0.28113163 0.         0.         0.28113163 0.35657982\n",
            "  0.         0.         0.         0.         0.35657982 0.\n",
            "  0.35657982 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ns-u1xHSY0V7"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}